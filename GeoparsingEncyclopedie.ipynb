{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrOKr9pwkxJw"
   },
   "source": [
    "# GEOPARSING HISTORICAL DOCUMENTS\n",
    "\n",
    "This notebook is proposed by [L. Moncla](https://ludovicmoncla.github.io/) and [K. McDonough](https://www.turing.ac.uk/people/researchers/katherine-mcdonough) as part of the [GéoDISCO](https://www.msh-lse.fr/projets/geodisco/) (2019-2020) and GEODE (2020-2024) projects.\n",
    "\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GEODE-project/perdido-geoparsing-notebook/master?filepath=GeoparsingEncyclopedie.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we'll learn about a few different things.\n",
    "\n",
    "- How to load data from TEI-XML files into a Python dataframe\n",
    "- Use Python dataframe for simple data analysis\n",
    "- Test the [PERDIDO API](http://erig.univ-pau.fr/PERDIDO/api.jsp) for preprocessing French texts (part-of-speech tagging)\n",
    "- Test the [PERDIDO API](http://erig.univ-pau.fr/PERDIDO/api.jsp) for geoparsing (geotagging + geocoding) Encyclopedie articles\n",
    "- Display custom geotagging results (PERDIDO TEI-XML) with the [displaCy Named Entity Visualizer](https://spacy.io/usage/visualizers)\n",
    "- Display geocoding results on a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gi1PFqtkxJy"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Geoparsing (also known as toponym resolution) refers to the process of extracting place names from text and assigning geographic coordinates to them.\n",
    "This involves two main tasks: geotagging and geocoding.\n",
    "Geotagging consists to identify spans of text referring to place names while geocoding consists to find unambiguous geographic coordinates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The PERDIDO Geoparser API \n",
    "\n",
    "The [PERDIDO API](http://erig.univ-pau.fr/PERDIDO/) has been developped for extracting and retrieving displacements from unstructured texts. It has initially been developed for French, Spanish and Italian hiking descriptions.\n",
    "\n",
    "More recently, as part of the [GéoDISCO project](https://www.msh-lse.fr/projets/geodisco/) we have developed a custom version for historical documents and more specifically for the Encyclopédie.\n",
    "\n",
    "\n",
    "In this tutorial we'll see how to use the PERDIDO API for preprocessing and geoparsing French texts. \n",
    "We will apply geoparsing on the Encyclopedie corpus version released by the [ARTFL project](https://encyclopedie.uchicago.edu/) and we'll show the limit of geotagging and geocoding historical documents.\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "Data courtesy the [ARTFL Encyclopédie Project](https://artfl-project.uchicago.edu/), University of Chicago.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "\n",
    "You need to register on the PERDIDO website to get your API key: http://erig.univ-pau.fr/PERDIDO/api.jsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kO1rgEo4kxJ1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import of python libraries\n",
    "\n",
    "import requests\n",
    "import lxml.etree as etree\n",
    "import xml.dom.minidom as xml\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.tokens import Span\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy import displacy\n",
    "\n",
    "from display_xml import XML  \n",
    "\n",
    "import geojson\n",
    "import folium\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9ZN6YgcnxdK"
   },
   "source": [
    "## 1. Loading the data\n",
    "\n",
    "Here we assume that we have access to a directory with the corpus of documents. \n",
    "In our case, documents are XML-TEI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9DgavARA8rLp",
    "outputId": "fd8751c0-1128-4a5c-bfe6-4d18b7a7eb81"
   },
   "outputs": [],
   "source": [
    "path = './data/' # path of the directory containing the corpus of documents\n",
    "\n",
    "# select one document for testing\n",
    "file = 'volume09-3630.tei' # Lyon: https://artflsrv03.uchicago.edu/philologic4/encyclopedie1117/navigate/9/3630/\n",
    "\n",
    "# get the XML-TEI content of the document\n",
    "root = etree.parse(path + file, etree.XMLParser(remove_blank_text=True)).getroot()\n",
    "\n",
    "# print the XML-TEI content\n",
    "print(xml.parseString(etree.tostring(root)).toprettyxml(indent='  ')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extracting metadata and content from XML-TEI\n",
    "\n",
    "In the following cell, we define a function for parsing and extracting metadata and text content from an XML-TEI file.\n",
    "In this example, we only extract from the metadata the normclass (classification of the article, e.g. 'Géographie'), the head (head word of the article), and the author of the article. Then, we also extract the textual content as raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCbcdeuDpuR9"
   },
   "outputs": [],
   "source": [
    "def getDataFromEDDATEI(file_path, filename):\n",
    "    file_id = filename[:-4]\n",
    "    d = []\n",
    "    try:\n",
    "        volume = filename[6:8] \n",
    "        number = filename[9:-4] \n",
    "        head = ''\n",
    "        normClass = ''\n",
    "        author = ''\n",
    "        txtContent = ''\n",
    "        root = etree.parse(file_path+filename).getroot()\n",
    "        div1 = root.find('./text/body/div1')\n",
    "        if len(div1):\n",
    "            for elt in div1:\n",
    "                if elt.tag == 'p':\n",
    "                    txtContent += ''.join(elt.itertext())\n",
    "                    txtContent = txtContent.replace('\\n', ' ').strip()\n",
    "                elif elt.tag == 'index':\n",
    "                    if elt.get('type') == 'normclass':\n",
    "                        normClass = elt.get('value')\n",
    "                    if elt.get('type') == 'head':\n",
    "                        head = elt.get('value')\n",
    "                    if elt.get('type') == 'author':\n",
    "                        author = elt.get('value')\n",
    "        d = [filename, volume, number, head, normClass, author, txtContent]\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        pass\n",
    "        #print(filename + ': ' + str(e))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a list object containing the filename, number and volume, the head word, the class, the author, and the textual content.\n",
    "Let see what is the result of this function for the article about Lyon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "fcDObW7gK1Fo",
    "outputId": "0ff4a85f-d8bf-4a3e-90a3-d472265c3d19"
   },
   "outputs": [],
   "source": [
    "getDataFromEDDATEI(path, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to easily analyse and use these data we will now load these information about all the documents in our directory into a [Python dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "-wLUfKY3nv1V",
    "outputId": "93c8b3b2-c984-4fad-b4d3-81b54e50bfa2"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for doc in os.listdir(path):\n",
    "    if doc[-4:] == '.tei':\n",
    "        data.append(getDataFromEDDATEI(path, doc))\n",
    "df = pd.DataFrame(data, columns=['filename', 'volume', 'number', 'head', 'normClass', 'author', 'txtContent'])\n",
    "df = df.dropna()\n",
    "df = df.sort_values(['volume', 'number']).reset_index(drop = True)\n",
    "\n",
    "df.head(10) # show the 10 first rows of the dataframe\n",
    "#df.tail(10) # show the 10 last rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have access to all the attributs and methods of the dataframe object. For instance, we can easily print the number of rows in our dataframe which correspond to the number of articles in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df.shape[0]\n",
    "print('There are ' + str(n) + ' articles in the input directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 First look at the data\n",
    "\n",
    "Now that the data from the XML-TEI files are loaded into a python dataframe, we can have a look at them.\n",
    "For instance, we can select articles based on their classification in the Encyclopedie.\n",
    "If we want all articles in 'geography' we can just do as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of class that refers to 'Géographie'\n",
    "normclassGEO = ['Géographie', 'Géographie moderne',\n",
    "                 'Géographie ancienne', 'Géographie moderne | Géographie ancienne',\n",
    "                 'Géographie ancienne | Géographie moderne', 'Géographie sacrée', 'Géographie sainte',\n",
    "                 'Géographie | Histoire ancienne', 'Géographie historique', 'Géographie | Histoire',\n",
    "                 'Histoire | Géographie', 'Géographie | Histoire naturelle', 'Géographie | Mythologie',\n",
    "                 'Géographie ancienne | Mythologie', 'Histoire moderne | Géographie',\n",
    "                 'Géographie ancienne | Géographie sainte', 'Géographie ancienne | Géographie sacrée',\n",
    "                 'Géographie sacrée | Géographie ancienne', 'Géographie du moyen âge', 'Géographie des Arabes',\n",
    "                 'Géographie | Commerce', 'Histoire | Géographie ancienne',\n",
    "                 'Géographie | Histoire ancienne | Histoire moderne', 'Géographie ancienne | Littérature | Histoire',\n",
    "                 'Histoire naturelle | Géographie', 'Géographie | Histoire ancienne | Mythologie',\n",
    "                 'Géographie moderne | Commerce', 'Géographie ancienne | Géographie antique',\n",
    "                 'Géographie moderne | Histoire', 'Géographie | Histoire monastique',\n",
    "                 'Géographie ancienne | Géographie moderne | Mythologie', 'Géographie ancienne | Histoire',\n",
    "                 'Géographie ancienne | Littérature | Mythologie', 'Géographie ancienne | Médailles'\n",
    "                 ]\n",
    "\n",
    "# query the dataframe for all articles matching one of the class in our list\n",
    "df_geo = df.loc[df['normClass'].isin(normclassGEO)]\n",
    "df_geo.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are ' + str(df_geo.shape[0]) + ' geography articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can also make a query based on the value of the data. For instance, we can query all the articles of a specific author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'Jaucourt'\n",
    "n = df_geo.loc[df['author'] == val].shape[0]\n",
    "print(str(n) + ' were written by '+ val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily show the number of articles per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo.groupby(['author'])[\"filename\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show the value of one of the column of our dataframe for a specific row (i.e., article) based on its name. For instance, if we want to know who wrote the article about Lyon or if we want to see its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['head'] == 'LYON'].author.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['head'] == 'LYON'].txtContent.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform a keyword search over the text content of all articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'france'\n",
    "df_2 = df[df['txtContent'].str.contains(val, case=False)]\n",
    "print(str(df_2.shape[0]) + ' articles contain the word \\''+ val + '\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example with the expression \"ville de\" will extract all articles that contain the expression 'ville de':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['txtContent'].str.contains(\"ville de\", case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same with the words 'océan pacifique' and 'mer pacifique'. Which can be used to study the extent of the Encyclopedie on the pacific area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['txtContent'].str.contains(\"océan pacifique|mer pacifique\", case=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the same with a more thematic search for instance about 'esclavage': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['txtContent'].str.contains(\"esclavage\", case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asi5amjhkxKA"
   },
   "source": [
    "### 1.3 Preprocessing text content\n",
    "\n",
    "#### Tokenization and part-of-speech (POS) tagging \n",
    "\n",
    "In Natural Language Processing (NLP), the main first steps before processing text content consist in tokenizing sentences and words and assigning to each word its grammatical category (Part-of-Speech). Then, this allows the construction of more complex rules or queries than a simple keyword search.\n",
    "This preprocessing step is language dependent and thus we have to choose the right tool according to the language of our documents. This is a major difficulty when dealing with historical or ancient texts. For instance, for French it is difficult to find a POS tagger for old French as all well known taggers are trained on contemporary corpora.\n",
    "\n",
    "> McDonough, K., Moncla, L., & van de Camp, M. (2019). Named entity recognition goes to old regime France: geographic text analysis for early modern French corpora. International Journal of Geographical Information Science, 33, 2498–2522.\n",
    "\n",
    "\n",
    "The PERDIDO API uses [Treetagger](https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/) for part-of-speech tagging. \n",
    "\n",
    "Let's have a first try of the PERDIDO API. We will first test the POS service which takes 3 parameters: the API key of the user, the language, and the text content. This service returns the annotated text in TEI-XML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSb_d66CkxKC"
   },
   "outputs": [],
   "source": [
    "api_key = 'demo' # !! replace by yours\n",
    "lang = 'French'  # currently only available for French\n",
    "version = 'Encyclopedie' # default: Standard (the standard version has been developped for the analysis of hiking descriptions)\n",
    "gazetteer = 'wikipedia'  # default: bdnyme_ign (only for France)\n",
    "\n",
    "content = df.loc[df['head'] == 'GANGEA'].txtContent.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "oUuZWCKOkxKI",
    "outputId": "26551acd-cead-4526-888f-a6055dc83462"
   },
   "outputs": [],
   "source": [
    "# set the parameters for the PERDIDO POS tagging service\n",
    "parameters = {'api_key': api_key, 'lang': lang, 'content': content}\n",
    "POS_WebService = 'http://erig.univ-pau.fr/PERDIDO/api/pos/txt_xml/'\n",
    "\n",
    "# run the PERDIDO POS service\n",
    "r = requests.get(POS_WebService, params=parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the processed is done, we can print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "RLD_kFeikxKQ",
    "outputId": "1c3b4b00-46a1-4b93-ad09-6ebc0da59fa4"
   },
   "outputs": [],
   "source": [
    "print(r.text) # shows the result of the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "CyiZokd2kxKV",
    "outputId": "557f4494-76ae-47a7-882a-340f1ba2b142"
   },
   "outputs": [],
   "source": [
    "print(xml.parseString(r.text).toprettyxml()) # shows the result in more a readable way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "Q8k1Wd2NkxKb",
    "outputId": "b8a5630a-a813-4e50-f11c-a07988750060",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XML(bytes(r.text, 'utf-8'), style='colorful') # shows the same result with syntax color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results we can notice that each word have been tokenised and annotated with the XML element <w> containing the attributes lemma and type (POS tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTcr95AokxKh"
   },
   "source": [
    "## 2. Geoparsing : geotagging + geocoding\n",
    "\n",
    "Geoparsing is divided into two main tasks: geotagging (NER) and geocoding.\n",
    "\n",
    "The geotagging service of the PERDIDO API uses a cascade of finite-state transducers defining specific patterns for NER and identification of geographic information (spatial relations, etc.). \n",
    "> Mauro Gaio and Ludovic Moncla (2019). “Geoparsing and geocoding places in a dynamic space context.“ In The Semantics of Dynamic Space in French: Descriptive, experimental and formal studies on motion expression, 66, 353.\n",
    "\n",
    "For our custom version of the PERDIDO Geoparser, the geocoding task uses a simple gazetteer lookup method. We use the French wikiGazetteer (a gazetteer based on Wikipedia and enriched with Geonames data) generated following this work: https://github.com/alan-turing-institute/lwm_GIR19_resolving_places/tree/master/gazetteer_construction\n",
    "> Mariona Coll Ardanuy, Katherine McDonough, Amrey Krause, Daniel CS Wilson, Kasra Hosseini, and Daniel van Strien. (2019) “Resolving Places, Past and Present: Toponym Resolution in Historical British Newspapers Using Multiple Resources”. In Proceedings of the 13th Workshop on Geographic Information Retrieval (GIR19).\n",
    "\n",
    "Geographic text analysis research in the digital humanities has focused on projects analyzing modern English-language corpora. \n",
    "In this tutorial we propose to highlight the difficulties of extracting and mapping geographical information from historical French texts.\n",
    "As we'll see in the following, in addition to the problem of language when it comes to historical documents, the early-modern period lacks temporally appropriate gazetteers.\n",
    "\n",
    "> McDonough, K., Moncla, L., & van de Camp, M. (2019). Named entity recognition goes to old regime France: geographic text analysis for early modern French corpora. International Journal of Geographical Information Science, 33, 2498–2522.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PERDIDO Geoparser\n",
    "\n",
    "The PERDIDO Geoparsing service (`http://erig.univ-pau.fr/PERDIDO/api/geoparsing/`) takes 6 new parameters:\n",
    "1. api_key: API key of the user\n",
    "2. lang: language of the document (currently only available for French)\n",
    "3. content: textual content to parse\n",
    "4. mode: indicates if the query uses exact match on the name (mode: *s*) or if it uses also alternate names (mode: *a*). (default : *s*)\n",
    "5. records_limit: maximum number of records found in gazetteer for each toponym (default: 1)\n",
    "6. version: indicates the version of the geoparser (Encyclopedie or Standard). Default: Standard (the standard version has been developped for the analysis of hiking descriptions)\n",
    "\n",
    "The PERDIDO Geoparser returns XML-TEI. The `<name>` element refers to named entities (proper nouns) and the type attribute indicates its class (place, person, etc.). The `<rs>` element refers to extended named entities (e.g. ville d'Egypte). The `<location>` element indicates that geographic coordinates were found during geocoding.  \n",
    "\n",
    "\n",
    "\n",
    "As we'll see in the next cell, when we apply the PERDIDO Geoparser to the following example: (Volume 1 article 5236, available online from the [ARTFL project](https://artflsrv03.uchicago.edu/philologic4/encyclopedie1117/navigate/1/5236/))\n",
    "\n",
    ">AZIRUTH (Géographie.) petite ville d'Egypte, sur la côte occidentale de la mer Rouge ; ce n'est presque plus qu'un village.\n",
    "\n",
    "\n",
    "Three spatial entities are found during geotagging:\n",
    "1. Aziruth, \n",
    "2. petite ville d'Egypte\n",
    "3. la côte occidentale de la mer Rouge\n",
    "\n",
    "while only one entity (*Egypte*) is found during geocoding:\n",
    "\n",
    "```xml\n",
    "<name type=\"place\" subtype=\"edda\" id=\"en.2\">\n",
    "   <w lemma=\"null\" type=\"NPr\" xml:id=\"w9\">Egypte</w>\n",
    "   <location>\n",
    "      <geo source=\"wiki\">35.4833 24.1333</geo>\n",
    "   </location>\n",
    "</name>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2PqCCV2hkxKi",
    "outputId": "ce0d0fcb-73d0-4a8e-f750-d7607147907d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set the parameters for the PERDIDO geoparsing service\n",
    "content = df.loc[df['head'] == 'AZIRUTH'].txtContent.item()\n",
    "parameters = {'api_key': api_key, 'lang': lang, 'content': content, 'mode': \"s\", \"records_limit\": 1, \"version\": version, \"gazetteer\": gazetteer}\n",
    "\n",
    "# request the PERDIDO API\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geoparsing/', params=parameters)\n",
    "\n",
    "display(XML(bytes(r.text, 'utf-8'), style='colorful')) # shows the PERDIDO-GEOPARSER XML output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ezZdv-gkxKn"
   },
   "source": [
    "In the next cells, we will use the displacy library from spaCy to display the PERDIDO-NER XML output. For this purpose, we define the function `Perdido2displaCy()` in order to transform the PERDIDO-NER XML into a [spaCy](https://spacy.io/) compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSy-arJGkxKn"
   },
   "outputs": [],
   "source": [
    "''' function Perdido2displaCy()\n",
    "    transforms the PERDIDO-NER XML output into spaCy format (for display purpose) '''\n",
    "def Perdido2displaCy(contentXML):\n",
    "    vocab = Vocab()\n",
    "    words = []\n",
    "    spaces = []\n",
    "    root = etree.fromstring(bytes(contentXML, 'utf-8'))\n",
    "    contentTXT = \"\"\n",
    "    for w in root.findall('.//w'):\n",
    "        contentTXT += w.text + ' '\n",
    "        words.append(w.text)\n",
    "        spaces.append(True)\n",
    "    doc = Doc(vocab, words=words, spaces=spaces)\n",
    "    ents = [] \n",
    "    for child in root.findall('.//rs'):\n",
    "        if not parent_exists(child, 'rs'):\n",
    "            if 'startT' in child.attrib:\n",
    "                start = child.get('startT')\n",
    "                if 'endT' in child.attrib:\n",
    "                    stop = child.get('endT')\n",
    "                    if 'type' in child.attrib:\n",
    "                        if child.get('type') == 'place':\n",
    "                            type = 'LOC'\n",
    "                        elif child.get('type') == 'person':\n",
    "                            type = 'PERSON'\n",
    "                        else:\n",
    "                            type = 'MISC'\n",
    "                    else:\n",
    "                        type = 'MISC'\n",
    "                    ents.append(Span(doc, int(start), int(stop), label=type))\n",
    "    doc.ents = ents\n",
    "    return doc \n",
    "\n",
    "''' function parent_exists() \n",
    "    returns True if one of the ancestor of the element child_node have the name name_node''' \n",
    "def parent_exists(child_node, name_node):\n",
    "    try:\n",
    "        parent_node = next(child_node.iterancestors())\n",
    "        if parent_node.tag == name_node:\n",
    "            if 'startT' in parent_node.attrib:\n",
    "                return True\n",
    "        return parent_exists(parent_node, name_node)\n",
    "    except StopIteration:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Z47bCdQgkxKt",
    "outputId": "e22446ab-fad7-4af0-d05d-07db97780792"
   },
   "outputs": [],
   "source": [
    "doc = Perdido2displaCy(r.text) # transforms the PERDIDO-GEOPARSER XML\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True) # shows the PERDIDO-GEOPARSER XML output using the displacy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7bw9rWGkxKx"
   },
   "source": [
    "### 2.2 Mapping place names from one article\n",
    "\n",
    "If you're interested in geocoding, you probably want to display the result on a map. \n",
    "There are two solutions, \n",
    "1. you can parse the PERDIDO-GEOPARSER XML and extract each `<location>` element in order to get lat/long coordinate of each entity\n",
    "2. you can use the PERDIDO-GEOCODING service `http://erig.univ-pau.fr/PERDIDO/api/geocoding/`. It takes the same parameters than the geoparsing service and returns the result as *geojson*.\n",
    "\n",
    "Let's try solution n°2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' function get_bounding_box() returns a list containing the bottom left and the top right \n",
    "    points in the sequence '''\n",
    "def get_bounding_box(points):\n",
    "    bot_left_x = min(point[1] for point in points)\n",
    "    bot_left_y = min(point[0] for point in points)\n",
    "    top_right_x = max(point[1] for point in points)\n",
    "    top_right_y = max(point[0] for point in points)\n",
    "    return [(bot_left_x, bot_left_y), (top_right_x, top_right_y)]\n",
    "\n",
    "''' function display_map() display the map using the folium library '''\n",
    "def display_map(json_data):\n",
    "    coords = list(geojson.utils.coords(json_data))\n",
    "    if len(coords) > 0:\n",
    "        print(str(len(coords))+\" records found in gazetteer:\")\n",
    "\n",
    "        m = folium.Map()\n",
    "        m.fit_bounds(get_bounding_box(coords), max_zoom=5)\n",
    "        folium.GeoJson(data, name='Toponyms', tooltip=folium.features.GeoJsonTooltip(fields=['id', 'name', 'source'], localize=True)).add_to(m)\n",
    "\n",
    "        display(m)\n",
    "    else:\n",
    "        print(\"Sorry, no records found in gazetteer for geocoding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ay5XDTM1kxKy",
    "outputId": "93df6728-dd7e-40c7-c2c5-fae62699a7c0"
   },
   "outputs": [],
   "source": [
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geocoding/', params=parameters)\n",
    "data = geojson.loads(r.text)\n",
    "\n",
    "\n",
    "display_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxNQv8CrkxK3"
   },
   "source": [
    "Now, let's change the parameters. For geocoding, we will now use alternate names and limit the maximum number of records to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3R0QQjD8kxK3",
    "outputId": "c64f134b-6dc6-46ba-a299-a3fe0014671a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "parameters = {'api_key': api_key, \n",
    "              'lang': lang, \n",
    "              'content': content, \n",
    "              'mode': 'a', \n",
    "              'records_limit': 5, \n",
    "              'version': version, \n",
    "              'gazetteer': gazetteer}\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geoparsing/', params=parameters)\n",
    "doc = Perdido2displaCy(r.text)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geocoding/', params=parameters)\n",
    "data = geojson.loads(r.text)\n",
    "\n",
    "display_map(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgXluQMOkxK-"
   },
   "source": [
    "Let's try again with a maximum number of record of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ywqVvPWJkxK_",
    "outputId": "939428ff-617b-447f-8387-9f5c53e1e658",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {'api_key': api_key, \n",
    "              'lang': lang, \n",
    "              'content': content, \n",
    "              'mode': 'a', \n",
    "              'records_limit': 1, \n",
    "              'version': version,\n",
    "              'gazetteer': gazetteer\n",
    "             }\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geoparsing/', params=parameters)\n",
    "doc = Perdido2displaCy(r.text)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geocoding/', params=parameters)\n",
    "data = geojson.loads(r.text)\n",
    "\n",
    "display_map(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N00hyq57kxLD"
   },
   "source": [
    "Sometimes place names can be found in the text but there may be no result for geocoding. This means that none of the entity have records found in gazetteer. This is often the case for historical documents and for periods for which no appropriate gazetteer exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "ZUGMzvX4kxLD",
    "outputId": "95593a30-2039-4304-a700-08969033c91b"
   },
   "outputs": [],
   "source": [
    "content = df.loc[df['head'] == 'AZMER'].txtContent.item()\n",
    "\n",
    "\n",
    "parameters = {'api_key': api_key, \n",
    "              'lang': lang, \n",
    "              'content': content, \n",
    "              'mode': 'a', \n",
    "              'records_limit': 1, \n",
    "              'version': version,\n",
    "              'gazetteer': gazetteer\n",
    "             }\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geoparsing/', params=parameters)\n",
    "doc = Perdido2displaCy(r.text)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geocoding/', params=parameters)\n",
    "data = geojson.loads(r.text)\n",
    "\n",
    "display_map(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bKA5n09_kxLH",
    "outputId": "21deb4d5-6f9a-4572-ebf5-f9c9db7647a9"
   },
   "outputs": [],
   "source": [
    "content = df_geo.loc[df_geo['head'] == 'DAUPHINE'].txtContent.item()\n",
    "\n",
    "parameters = {'api_key': api_key, \n",
    "              'lang': lang, \n",
    "              'content': content, \n",
    "              'mode': 's', \n",
    "              'records_limit': 1, \n",
    "              'version': version,\n",
    "              'gazetteer': gazetteer\n",
    "             }\n",
    "\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geoparsing/', params=parameters)\n",
    "doc = Perdido2displaCy(r.text)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geocoding/', params=parameters)\n",
    "data = geojson.loads(r.text)\n",
    "\n",
    "display_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-4_s4JekxLM"
   },
   "source": [
    "### 2.3 Mapping several articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pacifique = df_geo[df_geo['txtContent'].str.contains(\"océan pacifique|mer pacifique\", case=False)]\n",
    "df_pacifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ', '.join(df_pacifique['head'].tolist())\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'api_key': api_key, \n",
    "              'lang': lang, \n",
    "              'content': content, \n",
    "              'mode': 's', \n",
    "              'records_limit': 1, \n",
    "              'version': version,\n",
    "              'gazetteer': gazetteer\n",
    "             }\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geoparsing/', params=parameters)\n",
    "doc = Perdido2displaCy(r.text)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geocoding/', params=parameters)\n",
    "data = geojson.loads(r.text)\n",
    "\n",
    "display_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the parameter 'mode' to search also for alternate names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'api_key': api_key, \n",
    "              'lang': lang, \n",
    "              'content': content, \n",
    "              'mode': 'a', \n",
    "              'records_limit': 1, \n",
    "              'version': version,\n",
    "              'gazetteer': gazetteer\n",
    "             }\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geoparsing/', params=parameters)\n",
    "doc = Perdido2displaCy(r.text)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geocoding/', params=parameters)\n",
    "data = geojson.loads(r.text)\n",
    "\n",
    "display_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geocoding headwords of articles containing the word 'esclavage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esclavage = df_geo[df_geo['txtContent'].str.contains(\"esclavage\", case=False)].reset_index(drop = True)\n",
    "content = ', '.join(df_esclavage['head'].tolist())\n",
    "\n",
    "parameters = {'api_key': api_key, \n",
    "              'lang': lang, \n",
    "              'content': content, \n",
    "              'mode': 'a', \n",
    "              'records_limit': 1, \n",
    "              'version': version,\n",
    "              'gazetteer': gazetteer\n",
    "             }\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geoparsing/', params=parameters)\n",
    "doc = Perdido2displaCy(r.text)\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "\n",
    "r = requests.get('http://erig.univ-pau.fr/PERDIDO/api/geocoding/', params=parameters)\n",
    "data = geojson.loads(r.text)\n",
    "\n",
    "display_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Toponym disambiguation using network analysis\n",
    "\n",
    "In our work, we use this methodoly for constructing a network based on the citation of \"géographie\" articles between them.\n",
    "We proposed to use network analysis measures to establish an approximate location, defined by qualitative relations, for each named toponym in EDDA. Throwing a list of decontextualized toponyms at an external resource like Geonames is risky. We therefore hypothesize that defining meaningful links between places can provide essentialinformation to improve disambiguation (and potentially replace resolution as the end goal). We establish connections between places based on the citation of “headword” toponyms (those that appearas headwords of entries) in other EDDA entries.\n",
    "\n",
    ">Moncla, L., McDonough, K., Vigier, D., Joliveau, T., & Brenon, A. (2019). Toponym disambiguation in historical documents using network analysis of qualitative relationships. Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities, 1–4. Chicago, IL, USA.\n",
    "\n",
    "This method draws on relations in the corpus of EDDA articles, which improves disambiguation at a later stage with an external resource. We suggest the network as an alternative to geospatial representation, a useful proxy when no historical gazetteer exists for the source material's period. Our first experiments have shown that this approach goes beyond a simple text analysis and is able to find relations between toponyms that are not co-occurring in the same documents. Network relations are also usefully compared with disambiguated toponyms to evaluate geographical coverage, and the ways that geographical discourse is expressed, in historical texts.\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <img src=\"img/labels_indegree2.png\" width =\"500px\"> </td>\n",
    "    <td> <img src=\"img/nodes_betweenness+class2.png\" width =\"500px\" > </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Node and label size indicate in-degree centrality</td>\n",
    "    <td>Node size indicates betweenness centrality<br/> \n",
    "        colors refer to geographic feature types <br/> \n",
    "        (city: red, hydronym: blue, country: green, mountain: brown, unclassified: grey)</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "\n",
    "We also made somse preliminary tests by assigning geographic coordinates found in our French wikiGazetteer to each node (headword). We have only 2535 nodes with coordinates over the 13734 nodes. \n",
    "\n",
    "Our first experiment is shown below. Colors identify clusters of nodes computed with the [modularity measure](https://en.wikipedia.org/wiki/Modularity_(networks)) implemented on Gephy.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"img/geocodingEDDA1.png\" width =\"500\"> </td>\n",
    "<td> <img src=\"img/geocodingEDDA_network.png\" width =\"500\" > </td>\n",
    "</tr></table> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDgWiH2-kxMB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Geoparsing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
